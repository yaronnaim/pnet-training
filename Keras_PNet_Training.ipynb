{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Outputs\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACES_PATH = '../data/face_detection/faces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 500\n",
    "\n",
    "def read_pos_images():\n",
    "    #Read positive images:\n",
    "    path, __, filenames = next(os.walk(FACES_PATH+'pos_train/'))\n",
    "    file_count = training_size #len(filenames)\n",
    "    images = np.empty([0,12,3])\n",
    "    for i in range(file_count):\n",
    "        j=i+1\n",
    "        img=cv2.imread(f\"{path}{j}.bmp\")\n",
    "        images=np.append(images,img,axis=0)\n",
    "    #Create list of probabilities:\n",
    "    prob=[]\n",
    "    for i in range(file_count):\n",
    "        prob.append([[[0.0,1.0]]])\n",
    "    #Create list of coordinates:\n",
    "    coordinates=[]\n",
    "    file = open(FACES_PATH+'coordinates.txt','r')\n",
    "    lines = file.readlines()\n",
    "    lines = [line[:-1] for line in lines]\n",
    "    idx=[1,0,3,2]\n",
    "    for line in lines:\n",
    "        line = line.split(\" \")\n",
    "        line = line[1]\n",
    "        line=line[1:-1]\n",
    "        line = line.split(\",\")\n",
    "        #Transpose coordinates\n",
    "        x=0\n",
    "        nline=[]\n",
    "        for i in idx:\n",
    "            nline.append(line[i])\n",
    "            x=x+1\n",
    "        line=[[[float(c) for c in nline]]]\n",
    "        coordinates.append(line)\n",
    "    #Return images, probs, and coordinates\n",
    "    return images, prob, coordinates\n",
    "\n",
    "def read_neg_images():\n",
    "    #Read negative images:\n",
    "    path, __, filenames = next(os.walk(FACES_PATH+'neg_train/'))\n",
    "    file_count = training_size #len(filenames)\n",
    "    images = np.empty([0,12,3])\n",
    "    for i in range(file_count):\n",
    "        j=i+1\n",
    "        img=cv2.imread(f\"{path}{j}.bmp\")\n",
    "        images=np.append(images,img,axis=0)\n",
    "    #Create list of probabilities:\n",
    "    prob=[]\n",
    "    for i in range(file_count):\n",
    "        prob.append([[[1.0,0.0]]])\n",
    "    #Create list of coordinates:\n",
    "    coordinates=[]\n",
    "    for i in range(file_count):\n",
    "        coordinates.append([[[0.0,0.0,0.0,0.0]]])\n",
    "    #Return images, prob, coordinates\n",
    "    return images, prob, coordinates\n",
    "\n",
    "#Read in all images, probabilities, and coordinates\n",
    "pimages, pprob, pcoordinates = read_pos_images()\n",
    "nimages, nprob, ncoordinates = read_neg_images()\n",
    "o_images=np.append(pimages,nimages,axis=0)\n",
    "o_images=np.reshape(o_images,(-1,12,12,3))\n",
    "o_prob=pprob+nprob\n",
    "o_coordinates=pcoordinates+ncoordinates\n",
    "\n",
    "#Shuffle them up using an index\n",
    "idx=np.arange(len(o_prob))\n",
    "np.random.shuffle(idx)\n",
    "images=np.empty_like(o_images)\n",
    "c=0\n",
    "for i in idx:\n",
    "    images[c]=o_images[i]\n",
    "    c=c+1\n",
    "#images=(np.float32)(images-127.5)/128.0\n",
    "images=(np.float32)(images)/255\n",
    "\n",
    "#images = np.transpose(images, (0, 2, 1, 3)) #Transpose images\n",
    "prob=[]\n",
    "for i in idx:\n",
    "    prob.append(o_prob[i])\n",
    "coordinates=[]\n",
    "for i in idx:\n",
    "    coordinates.append(o_coordinates[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train , Image batch shape  (1000, 12, 12, 3)\n",
      "y_train , Classification ground true batch shape  (1000, 1, 1, 2)\n",
      "y_train , Coordinates ground true batch shape  (1000, 1, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print('X_train , Image batch shape ', images.shape)\n",
    "print('y_train , Classification ground true batch shape ' ,np.array(prob).shape)\n",
    "print('y_train , Coordinates ground true batch shape ', np.array(coordinates).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X_data for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape (1000, 12, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_data shape',X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create \"y_data\" for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.concatenate((np.array(prob), np.array(coordinates)), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_data shape (1000, 1, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "print('y_data shape',y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_data Classification shape (1000, 1, 1, 2)\n",
      "y_data Coordinate shape (1000, 1, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print('y_data Classification shape', y_data[:,:,:,:2].shape)\n",
    "print('y_data Coordinate shape',y_data[:,:,:,2:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to \"train', \"val\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(X, y, training_prec = 0.7, val_prec = 0.1, test_prec = 0.2):\n",
    "        data_length = len(X)\n",
    "        num_training = np.int(data_length * training_prec)\n",
    "        num_validation = np.int(data_length * val_prec)\n",
    "        \n",
    "        mask = range(num_training)\n",
    "        X_train = X[mask]\n",
    "        y_train = y[mask]\n",
    "        mask = range(num_training, num_training + num_validation)\n",
    "        X_val = X[mask]\n",
    "        y_val = y[mask]\n",
    "        mask = range(num_training + num_validation, data_length)\n",
    "        X_test = X[mask]\n",
    "        y_test = y[mask]\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (700, 12, 12, 3)\n",
      "Train labels shape:  (700, 1, 1, 6) float64\n",
      "Validation data shape:  (100, 12, 12, 3)\n",
      "Validation labels shape:  (100, 1, 1, 6)\n",
      "Test data shape:  (200, 12, 12, 3)\n",
      "Test labels shape:  (200, 1, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data(X_data, y_data)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build P-Net Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling2D, Conv2D, Input, Layer, Concatenate, concatenate\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "#from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNet():\n",
    "    \n",
    "    #initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
    "    \n",
    "    #input layer\n",
    "    visible = Input(shape=(12,12,3))\n",
    "    conv1 = Conv2D(10, kernel_size=(3,3))(visible)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    prelu1 = PReLU(alpha_initializer ='zero', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(16, kernel_size=(3,3))(prelu1)\n",
    "    prelu2 = PReLU(alpha_initializer='zero', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(32, kernel_size=(3,3),)(conv2)\n",
    "    prelu3 = PReLU(alpha_initializer='zero', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(conv3)\n",
    "    \n",
    "    output1 = Conv2D(2, kernel_size=(1,1), activation='softmax')(conv3)\n",
    "    output2 = Conv2D(4, kernel_size=(1,1))(conv3)\n",
    "    \n",
    "    #output = tf.concat((output1, output2), axis=3)\n",
    "    output = concatenate([output1, output2], axis=-1)\n",
    "    \n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "                  \n",
    "    #compute the loss function over the classification and over bounding box \n",
    "    classification_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    bbox_loss = tf.keras.losses.MeanSquaredError() \n",
    "    \n",
    "    # Use the model function to build the forward pass.\n",
    "\n",
    "    #Define custom loss\n",
    "    def custom_loss(calss_score, bbox_score):\n",
    "\n",
    "        # Create a loss function for P-Net\n",
    "        def loss(y_true,y_pred):\n",
    "            prediction_loss = classification_loss(y_true[:,:,:,:2], calss_score)\n",
    "            coordinate_loss = bbox_loss(y_true[:,:,:,2:], bbox_score)\n",
    "            return prediction_loss + 0.5 * coordinate_loss * y_true[:,:,:,1]\n",
    "\n",
    "   \n",
    "        # Return a function\n",
    "        return loss\n",
    "    \n",
    "    learning_rate = 1e-3\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss = custom_loss(output1, output2),\n",
    "                  metrics=['accuracy'])\n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "    # plot graph\n",
    "    plot_model(model, to_file='multiple_outputs.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           (None, 12, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 10, 10, 10)   280         input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 5, 5, 10)     0           conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_121 (PReLU)             (None, 5, 5, 10)     10          max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 3, 3, 16)     1456        p_re_lu_121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 1, 1, 32)     4640        conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 1, 1, 2)      66          conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 1, 1, 4)      132         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 1, 6)      0           conv2d_204[0][0]                 \n",
      "                                                                 conv2d_205[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,584\n",
      "Trainable params: 6,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = PNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 0.7432 - accuracy: 0.4300 - val_loss: 0.6834 - val_accuracy: 0.1100\n",
      "Epoch 2/10\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.6587 - accuracy: 0.0357 - val_loss: 0.6455 - val_accuracy: 0.0800\n",
      "Epoch 3/10\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.6050 - accuracy: 0.0643 - val_loss: 0.6019 - val_accuracy: 0.2100\n",
      "Epoch 4/10\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.5489 - accuracy: 0.0757 - val_loss: 0.5603 - val_accuracy: 0.1700\n",
      "Epoch 5/10\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4963 - accuracy: 0.0671 - val_loss: 0.5329 - val_accuracy: 0.2100\n",
      "Epoch 6/10\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4557 - accuracy: 0.1357 - val_loss: 0.4849 - val_accuracy: 0.2700\n",
      "Epoch 7/10\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4062 - accuracy: 0.1757 - val_loss: 0.5060 - val_accuracy: 0.3300\n",
      "Epoch 8/10\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4215 - accuracy: 0.2900 - val_loss: 0.5505 - val_accuracy: 0.4300\n",
      "Epoch 9/10\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4005 - accuracy: 0.3914 - val_loss: 0.4019 - val_accuracy: 0.4000\n",
      "Epoch 10/10\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.3573 - accuracy: 0.3971 - val_loss: 0.3775 - val_accuracy: 0.3800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2002267bb88>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the PNet  to ensure that the implementation does not crash and produces outputs of the expected shape.\n",
    "Pnet will output are:\n",
    "1. Face classification,  size (batch,1,1,2) for 2 calss classification, \"Face\", and \"Not face\"\n",
    "2. Bounding box  (batch,1,1,4) for 4 boundind box corrdinates (x,y,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
