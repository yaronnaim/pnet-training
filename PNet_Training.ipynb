{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "import os\n",
    "\n",
    "FACES_PATH = '../data/face_detection/faces/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__(name=\"PNet\")\n",
    "        # Define layers here.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(10, (3, 3), name=\"conv1\")\n",
    "        self.prelu1 = tf.keras.layers.PReLU(tf.constant_initializer(0.25), shared_axes=[1, 2], name=\"prelu1\")\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool1\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(16, (3, 3), name=\"conv2\")\n",
    "        self.prelu2 = tf.keras.layers.PReLU(tf.constant_initializer(0.25), shared_axes=[1, 2], name=\"prelu2\")\n",
    "        self.conv3 = tf.keras.layers.Conv2D(32, (3, 3), name=\"conv3\")\n",
    "        self.prelu3 = tf.keras.layers.PReLU(tf.constant_initializer(0.25), shared_axes=[1, 2], name=\"prelu3\")\n",
    "        self.cls_output = tf.keras.layers.Conv2D(2, (1, 1), activation=\"softmax\", name=\"conv4-1\")\n",
    "        self.bbox_pred = tf.keras.layers.Conv2D(4, (1, 1), name=\"conv4-2\")\n",
    "        #self.landmark_pred = keras.layers.Conv2D(10, (1, 1), name=\"conv4_3\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define your forward pass here,\n",
    "        # using layers you previously defined (in `__init__`).\n",
    "        scores = None\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        scores = [self.cls_output(x), self.bbox_pred(x)]#, self.landmark_pred(x)]\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Keras mlti-output P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Outputs\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling2D, Conv2D, Input\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "#from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the PNet  to ensure that the implementation does not crash and produces outputs of the expected shape.\n",
    "Pnet will output are:\n",
    "1. Face classification,  size (batch,1,1,2) for 2 calss classification, \"Face\", and \"Not face\"\n",
    "2. Bounding box  (batch,1,1,4) for 4 boundind box corrdinates (x,y,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PNet(batch=64):    \n",
    "    model = PNet()\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((batch, 12, 12, 3))\n",
    "        classification_scores, bbox_score = model(x)\n",
    "        print(model.summary())\n",
    "        print('\\nP-Net output size testing: \\nclassificatin score output', classification_scores.shape,\n",
    "              '\\nbounsing box score output', bbox_score.shape)\n",
    "\n",
    "batch_test = 32\n",
    "test_PNet(batch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 500\n",
    "\n",
    "def read_pos_images():\n",
    "    #Read positive images:\n",
    "    path, __, filenames = next(os.walk(FACES_PATH+'pos_train/'))\n",
    "    file_count = training_size #len(filenames)\n",
    "    images = np.empty([0,12,3])\n",
    "    for i in range(file_count):\n",
    "        j=i+1\n",
    "        img=cv2.imread(f\"{path}{j}.bmp\")\n",
    "        images=np.append(images,img,axis=0)\n",
    "    #Create list of probabilities:\n",
    "    prob=[]\n",
    "    for i in range(file_count):\n",
    "        prob.append([[[0.0,1.0]]])\n",
    "    #Create list of coordinates:\n",
    "    coordinates=[]\n",
    "    file = open(FACES_PATH+'coordinates.txt','r')\n",
    "    lines = file.readlines()\n",
    "    lines = [line[:-1] for line in lines]\n",
    "    idx=[1,0,3,2]\n",
    "    for line in lines:\n",
    "        line = line.split(\" \")\n",
    "        line = line[1]\n",
    "        line=line[1:-1]\n",
    "        line = line.split(\",\")\n",
    "        #Transpose coordinates\n",
    "        x=0\n",
    "        nline=[]\n",
    "        for i in idx:\n",
    "            nline.append(line[i])\n",
    "            x=x+1\n",
    "        line=[[[float(c) for c in nline]]]\n",
    "        coordinates.append(line)\n",
    "    #Return images, probs, and coordinates\n",
    "    return images, prob, coordinates\n",
    "\n",
    "def read_neg_images():\n",
    "    #Read negative images:\n",
    "    path, __, filenames = next(os.walk(FACES_PATH+'neg_train/'))\n",
    "    file_count = training_size #len(filenames)\n",
    "    images = np.empty([0,12,3])\n",
    "    for i in range(file_count):\n",
    "        j=i+1\n",
    "        img=cv2.imread(f\"{path}{j}.bmp\")\n",
    "        images=np.append(images,img,axis=0)\n",
    "    #Create list of probabilities:\n",
    "    prob=[]\n",
    "    for i in range(file_count):\n",
    "        prob.append([[[1.0,0.0]]])\n",
    "    #Create list of coordinates:\n",
    "    coordinates=[]\n",
    "    for i in range(file_count):\n",
    "        coordinates.append([[[0.0,0.0,0.0,0.0]]])\n",
    "    #Return images, prob, coordinates\n",
    "    return images, prob, coordinates\n",
    "\n",
    "#Read in all images, probabilities, and coordinates\n",
    "pimages, pprob, pcoordinates = read_pos_images()\n",
    "nimages, nprob, ncoordinates = read_neg_images()\n",
    "o_images=np.append(pimages,nimages,axis=0)\n",
    "o_images=np.reshape(o_images,(-1,12,12,3))\n",
    "o_prob=pprob+nprob\n",
    "o_coordinates=pcoordinates+ncoordinates\n",
    "\n",
    "#Shuffle them up using an index\n",
    "idx=np.arange(len(o_prob))\n",
    "np.random.shuffle(idx)\n",
    "images=np.empty_like(o_images)\n",
    "c=0\n",
    "for i in idx:\n",
    "    images[c]=o_images[i]\n",
    "    c=c+1\n",
    "#images=(np.float32)(images-127.5)/128.0\n",
    "images=(np.float32)(images)/255\n",
    "\n",
    "#images = np.transpose(images, (0, 2, 1, 3)) #Transpose images\n",
    "prob=[]\n",
    "for i in idx:\n",
    "    prob.append(o_prob[i])\n",
    "coordinates=[]\n",
    "for i in idx:\n",
    "    coordinates.append(o_coordinates[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train , Image batch shape ', images.shape)\n",
    "print('y_train , Classification ground true batch shape ' ,np.array(prob).shape)\n",
    "print('y_train , Coordinates ground true batch shape ', np.array(coordinates).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X_data for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_data shape',X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create \"y_data\" for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.concatenate((np.array(prob), np.array(coordinates)), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_data shape',y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('y_data Classification shape', y_data[:,:,:,:2].shape)\n",
    "print('y_data Coordinate shape',y_data[:,:,:,2:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to \"train', \"val\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(X, y, training_prec = 0.7, val_prec = 0.1, test_prec = 0.2):\n",
    "        data_length = len(X)\n",
    "        num_training = np.int(data_length * training_prec)\n",
    "        num_validation = np.int(data_length * val_prec)\n",
    "        \n",
    "        mask = range(num_training)\n",
    "        X_train = X[mask]\n",
    "        y_train = y[mask]\n",
    "        mask = range(num_training, num_training + num_validation)\n",
    "        X_val = X[mask]\n",
    "        y_val = y[mask]\n",
    "        mask = range(num_training + num_validation, data_length)\n",
    "        X_test = X[mask]\n",
    "        y_test = y[mask]\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data(X_data, y_data)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets test a single batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "    #return tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PNet()\n",
    "optimizer = optimizer_init_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "bbox_loss = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.BinaryCrossentropy(name='train_classification_loss')\n",
    "train_bbox_loss = tf.keras.metrics.MeanSquaredError(name='train_bbox_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_batch = X_train[:32,:,:,:]\n",
    "y_train_batch = y_train[:32,:,:,:]\n",
    "\n",
    "# Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "train_loss.reset_states()\n",
    "train_accuracy.reset_states()\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    classification_scores, bbox_scores = model(X_train_batch , training=True)\n",
    "    prediction_loss = classification_loss(y_train_batch[:,:,:,:2], classification_scores)\n",
    "    coordinate_loss = bbox_loss(y_train_batch[:,:,:,2:], bbox_scores)\n",
    "    loss = prediction_loss + 0.5 * coordinate_loss\n",
    "    # Print loss \n",
    "    print('Classification loss',prediction_loss)\n",
    "    print(coordinate_loss)\n",
    "    print(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update the metrics\n",
    "    train_loss.update_state(y_train_batch[:,:,:,:2], classification_scores)\n",
    "    train_bbox_loss.update_state(y_train_batch[:,:,:,2:], bbox_scores)\n",
    "    train_accuracy.update_state(y_train_batch[:,:,:,:2], classification_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'Loss: {}, Accuracy: {}'\n",
    "print (template.format(train_loss.result(),\n",
    "                       train_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='Reward/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on  training set and periodically checks\n",
    "    accuracy on the validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "        #compute the loss function over the classification and ovr bounding box \n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        bbox_loss = tf.keras.losses.MeanSquaredError()        \n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.BinaryCrossentropy(name='train_classification_loss')\n",
    "        train_bbox_loss = tf.keras.metrics.MeanSquaredError(name='train_bbox_loss')\n",
    "        \n",
    "        train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "            \n",
    "        #val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_loss = tf.keras.metrics.BinaryCrossentropy(name='val_classification_loss')\n",
    "        val_bbox_loss = tf.keras.metrics.MeanSquaredError(name='val_bbox_loss')\n",
    "\n",
    "        val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_bbox_loss.reset_states()\n",
    "            \n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    classification_scores, bbox_scores = model(x_np, training=True)\n",
    "                    prediction_loss = classification_loss(y_np[:,:,:,:2], classification_scores)\n",
    "                    coordinate_loss = bbox_loss(y_np[:,:,:,2:], bbox_scores)\n",
    "                    loss = prediction_loss + 0.5 * coordinate_loss * y_np[:,:,:,1]\n",
    "                    # Print loss \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(y_np[:,:,:,:2], classification_scores)\n",
    "                    train_bbox_loss.update_state(y_np[:,:,:,2:], bbox_scores*y_np[:,:,:,1] )\n",
    "                    train_accuracy.update_state(y_np[:,:,:,:2], classification_scores)\n",
    "\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_bbox_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            classification_scores, bbox_scores = model(test_x, training=False)\n",
    "                            t_prediction_loss = classification_loss(test_y[:,:,:,:2], classification_scores)\n",
    "                            t_coordinate_loss = bbox_loss(test_y[:,:,:,2:], bbox_scores)\n",
    "                            t_loss = t_prediction_loss + 0.5 * t_coordinate_loss * test_y[:,:,:,1]\n",
    "\n",
    "                            val_loss.update_state(test_y[:,:,:,:2], classification_scores)\n",
    "                            val_bbox_loss.update_state(test_y[:,:,:,2:], bbox_scores*test_y[:,:,:,1])\n",
    "                            val_accuracy.update_state(test_y[:,:,:,:2], classification_scores)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, \\nLoss: {}, Bbox loss: {}, Accuracy: {},\\nVal Loss: {}, Val Bbox Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             loss,\n",
    "                                             train_bbox_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_bbox_loss.result(),  \n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 10\n",
    "num_epochs = 5\n",
    "\n",
    "#model = PNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return PNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "    #return tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = train(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_face = X_test[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(test_face, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_face.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(0.4,0.4))\n",
    "plt.imshow(test_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
